x<!DOCTYPE html>
<html lang="en">
<head>
<link href='https://fonts.googleapis.com/css?family=Roboto:500,300&subset=latin' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="style.css">
<link rel="icon"
      type="image/png"
      href="img/favicon.ico">
<title>Anita Ver&#337;</title>
<!-- <script>

</script> -->
</head>
<body>


<div class="header">

	<div class="hcontent">

		<div class="txt-panel">
			<h1>Anita Lilla Ver&#337;</h1>
			<p>
			<i><strong>Senior ML Research Engineer, Singer, Writer</strong></i><br/>
			<small>anitaveroe _at_ gmail.com</small></p><br/>
			<a href="https://www.linkedin.com/in/anita-lilla-vero/" target="_blank">
				<img class="icon" src="img/linkedin-white.png" alt="likedin">
			</a>
			<a href="https://github.com/anitavero" target="_blank">
				<img class="github-icon" src="img/GitHub_Logo.png" alt="github">
			</a>
			<a href="https://medium.com/@anilill" target="_blank">
				<img class="medium-icon" src="img/medium_logo.png" alt="medium">
			</a>
			<a href="https://twitter.com/anitaveroe" target="_blank">
				<img class="twitter-icon" src="img/Twitter social icons - circle - blue.png" alt="twitter">
			</a>
			<a href="https://scholar.google.com/citations?user=yIHiSswAAAAJ" target="_blank">
				<img class="twitter-icon" src="img/icons8-google-scholar.svg" alt="google scholar">
			</a>
		</div>

		<!-- <a href="http://www.cam.ac.uk/" target="_blank"><img class="logo" src="img/cambridge.png" alt="profile"/></a> -->
		<div class="round-pic" style="cursor: pointer;" onclick="window.location='index.html';"></div>

	</div> <!-- hcontent -->

</div> <!-- header -->

<!-- <hr /> -->

<div class="container" style="margin-top: 5px">
	<div class='nav-bar'>
    <!-- <a href="#bio">About me</a> &nbsp;  &nbsp; -->
	<a href="#interests">Interests</a> &nbsp;  &nbsp;
	<a href="#pubs">Publications</a> &nbsp;  &nbsp;
	<a href="#software">Software</a> &nbsp;  &nbsp;
	<a href="#teaching">Teaching</a> &nbsp;  &nbsp;
	<a href="#communal">Communal</a> &nbsp; &nbsp;
	<a href="#awards">Awards</a> &nbsp;  &nbsp;
	<a href="#music_lit">Music / Literature</a>
	</div>
</div>  <!-- container -->

<hr class="soft"/>

<div class="container">
	<h2><a class="section" id="bio">About me</a></h2>

	<p>I am a Senior Machine Learning Research Engineer and manager at <a href="https://www.unitary.ai/" target="_blank">Unitary</a>. I work on building multimodal video undersanding models for online safety and more. I often <a href="https://anilill.medium.com/" target="_blank">write about</a> responsible AI.</p>

	<p>Before this I did a PhD in Multimodal NLP in the <a href="http://www.cl.cam.ac.uk/research/nl/" target="_blank">Natural Language and Information Processing Group</a>, at the <a href="http://www.cl.cam.ac.uk/" target="_blank">University of Cambridge Computer Laboratory</a>, supervised by <a href="https://www.cl.cam.ac.uk/~aac10/" target="_blank">Prof Ann Copestake</a>, previously by <a href="https://sites.google.com/site/stephenclark609/" target="_blank">Dr. Stephen Clark</a>. I am a member of <a href="http://www.kings.cam.ac.uk/" target="_blank">King's College</a>.
	</p>

	<p>I did my undergraduate and masters degrees in computer science at the <a href="http://www.elte.hu/en" target="_blank">E&ouml;tv&ouml;s Lor&aacute;nt University</a>.
	During my masters studies and for one more year I have been working at the <a href="http://nipg.inf.elte.hu/" target="_blank">Neural Information Processing Group</a>. During this period I spent two months as a guest researcher at the <a href="http://www.dfki.de/web?set_language=en&cl=en" target="_blank">German Research Center for Artificial Intelligence</a> in Saarbr&uuml;cken, Germany.
	</p>

	<p>
	When I'm not doing research I am working on <a href="#music_lit">music and writing</a> projects, hiking, cycling or doing yoga.
	</p>

	<p>
	See my <a href="Anita_Vero_CV.pdf"  target="_blank">CV</a> for a description of my educational background and work experience.
	</p>
</div>  <!-- container -->

<hr/>

<div class="container">
	<h2><a class="section" id="interests">Interests</a></h2>
	<div class="holder">
	<img class="holder_img" src="img/analysis_scheme.png" alt="analysis scheme"
	style=" width: 40%; /* Will shrink image to 30% of its original width */
			    height: auto; 
			    float: right;
  			    padding-left: 30px;">
		<p><strong><a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-970.html">PhD Thesis</a></strong> -- My PhD research lies in the intersection of machine learning, natural language processing, computer vision and multi-modal semantic grounding. It's focus is on a deeper analysis of data sources and models, unearthing how different meaning representations "understand" concepts. The aim is to make a step towards interpretable models and to move beyond standard evaluation metrics such as accuracy.</p>

		<p><strong>Responsible AI</strong> -- As in the past few years AI technology has become ubiquitous in a large part of the world (and continues conquering more) I have become painfully aware of the complicated and often elusive side-effects of it on society. Beside crucial questions, such as data privacy, fake news and harmful content on the internet, I am preoccupied with how social media recommendation algorithms are merging with our minds in a feedback-loop, where we feed them with our choices and they mould our taste in return.
		See my <a href="https://anilill.medium.com/" target="_blank">blog</a> where I write about these issues as well.</p>
	</div>

</div>  <!-- container -->

<hr />

<div class="container">
	<h2><a class="section" id="pubs">Publications</a></h2>

	<ul>
		<li><strong>Language as the Medium: Multimodal Video Classification through text only</strong><br/>
		  Laura Hanu, <u>Anita L. Ver&#337;</u>, James Thewlis<br/>
			<i>ICCV 2023</i>, Paris, France, 
			<a href="https://www.rsipvision.com/ComputerVisionNews-2023November/28/" target="_blank">Best of ICCV 2023 Magazine</a><br/>
			[<a href="https://arxiv.org/pdf/2309.10783.pdf">paper</a>]
			[<a href="iccv_poster.pdf">poster</a>]
			[<a href="">code (TBA)</a>]
		</li>
		<li><strong>Transparent analysis of multi-modal embeddings (PhD Thesis)</strong><br/>
		  <u>Anita L. Ver&#337;</u><br/>
			<i>University of Cambridge, Computer Laboratory</i>, 2022 <br />
			[<a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-970.html">Techical report</a>]
		</li>
		<li><strong>Efficient Multi-Modal Embeddings from Structured Data</strong><br/>
		  <u>Anita L. Ver&#337;</u> and Ann Copestake<br/>
			<i>arXiv preprint arXiv:2110.02577</i>, 2021 <br />
			[<a href="https://arxiv.org/abs/2110.02577">paper</a>]
			[<a href="poster_Oxbridge2022.pdf">poster</a>]
		</li>
		<li><strong>Deconstructing Multimodality: Visual Properties and Visual Context in Human Semantic Processing</strong><br/>
		  Christopher Davis, Luana Bulat,  <u>Anita L. Ver&#337;</u> and Ekaterina Shutova<br/>
			<i>Proceedings of *SEM2019 2019</i>, Minneapolis, USA<br />
			[<a href="https://www.aclweb.org/anthology/S19-1013">paper</a>]
			[<a href="https://www.aclweb.org/anthology/S19-1013.bib">bib</a>]
		</li>
		<li><strong>Modelling Visual Properties and Visual Context in Multimodal Semantics</strong><br/>
		  Christopher Davis, Luana Bulat, <u>Anita L. Ver&#337;</u> and Ekaterina Shutova<br/>
		  <i>Workshop on Visually Grounded Interaction and Language, NIPS 2018</i>, Montreal, Canada<br />
			[<a href="https://nips2018vigil.github.io/static/papers/accepted/1.pdf">paper</a>]
		</li>
		<li><strong>Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research</strong><br/>
		  Douwe Kiela, Luana Bulat,  <u>Anita L. Ver&#337;</u> and Stephen Clark<br/>
		  <i>NIPS Workshop on Machine Intelligence (MAIN) 2016</i>, Barcelona, Spain, 2016<br />
			[<a href="https://arxiv.org/abs/1610.07432">paper</a>]
		</li>
		<li><strong>Comparing Data Sources and Architectures for Deep Visual Representation Learning in Semantics</strong><br/>
		  Douwe Kiela, <u>Anita L. Ver&#337;</u> and Stephen Clark<br/>
		<i>Proceedings of the Empirical Methods in Natural Language Processing Conference (EMNLP 2016)</i>, Austin, Texas, 2016<br />
			[<a href="https://aclweb.org/anthology/D/D16/D16-1043.pdf">paper</a>]
			[<a href="bibs/kiela-etal-2016-comparing.bib">bib</a>]
			[<a href="emnlp2016_poster.pdf">poster</a>]
		</li>
		<li><strong>Columnar Machine: Fast estimation of structured sparse code</strong><br/>
			Andr&aacute;s L&#337;rincz, Zolt&aacute;n &Aacute; Milacski, Bal&aacute;zs Pint&eacute;r, <u>Anita L. Ver&#337;</u><br/>
			<i>Biologically Inspired Cognitive Architectures, 2015</i><br />
			[<a href="http://www.sciencedirect.com/science/article/pii/S2212683X15000638">paper</a>]
		</li>
		<li><strong>Maintain and Improve Mental Health by Smart Virtual Reality Serious Games</strong><br/>
			Andr&aacute;s S&aacute;rk&aacute;ny, Zolt&aacute;n T&#337;s&eacute;r, <u>Anita L. Ver&#337;</u>, Andr&aacute;s L&#337;rincz, Takumi Toyama, Daniel Sonntag.<br/>
			<i>Proceedings of the 5th International Symposium on Pervasive Computing Paradigms for Mental Health, LNCS Spinger, 2015</i>
		</li>
		<li><strong>
			Towards a Smart Wearable Tool to Enable People with SSPI to Communicate by Sentence Fragments</strong><br/>
			Gyula V&ouml;r&ouml;s, <u>Anita L. Ver&#337;</u>, Takumi Toyama, Andr&aacute;s L&#337;rincz, Bal&aacute;zs Pint&eacute;r, Brigitta Miksztai-R&eacute;they, Daniel Sonntag.<br/>
			<i>Proceedings of the International Symposium on Pervasive Computing Paradigms for Mental Health, LNCS Spinger, 2014</i><br />
			[<a href="https://link.springer.com/chapter/10.1007/978-3-319-11564-1_10">paper</a>]
		</li>
		<li><strong>
			Mobile AAC Solutions using Gaze Tracking and Optical Character Recognition</strong><br/>
			Gyula V&ouml;r&ouml;s, Brigitta Miksztai-R&eacute;they, <u>Anita L. Ver&#337;</u>, Takumi Toyama, Jason Orlosky, Daniel Sonntag, Andr&aacute;s L&#337;rincz.<br/>
			<i>	16th Biennial Conference of the International Society for Augmentative and Alternative Communication (ISAAC), 2014</i>
		</li>
		<li><strong>
			Gaze Tracking and Language Model for Flexible Augmentative and Alternative Communication in Practical Scenarios</strong><br/>
			<u>Anita L. Ver&#337;</u>, Brigitta Miksztai-R&eacute;they, Gyula V&ouml;r&ouml;s, &Aacute;d&aacute;m Zsigmond, Bal&aacute;zs Pint&eacute;r, Takumi Toyama, Jason Orlosky, Daniel Sonntag, Andr&aacute;s L&#337;rincz.<br/>
			<i>16th Biennial Conference of the International Society for Augmentative and Alternative Communication (ISAAC), 2014</i>
		</li>
	</ul>

</div> <!-- container -->

<hr />

<div class="container">
	<h2><a class="section" id="software">Software</a></h2>

	<div class="holder">
    	<img class="holder_img" src="img/E_L_20_clusters.png" alt="Wikipedia clusters"
    	style=" width: 30%; /* Will shrink image to 30% of its original width */
  			    height: auto; 
  			    float: right;
			    padding-top: 30px;
  			    padding-left: 30px;">
		<h3><b><a href='https://github.com/anitavero/embeval' target="_blank">EmbEval</a></b></h3>
		<p>
		A Python toolkit for transparent and interpretable evaluation of Embeddings of different sources and modalities.
	    </p>
	</div>

	<div class="holder">
	<img class="holder_img" src="img/twitter_income_bias.png" alt="Twitter income bias"
	style=" width: 50%; /* Will shrink image to 30% of its original width */
			    height: auto; 
			    float: right;
			    padding-top: 30px;
			    padding-left: 30px;">
		<h3><b><a href='https://github.com/anitavero/twitter_thumbnail' target="_blank">Twitter Global Class Bias</a></b></h3>
	    <p>
		My submission to the first <a href='https://hackerone.com/twitter-algorithmic-bias?type=team' target="_blank">Twitter Algorithmic Bias Challenge</a>.
		I tested Twitter's image thumbnail cropping the algorithm for images of cheap versus expensive objects, using the income labels as proxy in the dataset of <a href='https://www.gapminder.org/dollar-street' target="_blank">Dollar Street</a>. I found that the algorithm is biased towards cheap rooms and spaces and towards expensive objects.

		See my <a href='https://www.cl.cam.ac.uk/~alv34/Twitter_Algorithmic_Bias___Report.pdf' target="_blank">report</a> on the detailed findings.

    	</p>
    </div>

	<div class="holder">
    	<img class="holder_img" src="img/concept_game.png" alt="Concept Game"
    	style=" width: 50%; /* Will shrink image to 30% of its original width */
  			    height: auto; 
  			    float: right;
			    padding-top: 30px;
  			    padding-left: 30px;">
	    <h3><b><a href='http://concept-guessing-game.com/' target="_blank">Concept Game</a></b></h3>
	   	<p>
	    is a two player collaborative game, where players guess the concept for a list of words. It is similar to the famous gamified data collection game <a href='https://en.wikipedia.org/wiki/ESP_game' target="_blank">ESP Game</a>, but with word clusters instead of images.
	    </p>
	    <p>The method used to generate the puzzles will be part of my PhD Thesis. Coming soon...</p>
	    <p>The game has been developed with the help of my friend, <a href='http://krisoft.hu' target="_blank">Kriszti√°n Gergely.</a></p>
	    <p>The code is available on <a href='https://github.com/anitavero/concept_game' target="_blank">Github</a></p>

	    <h4><a href='http://concept-guessing-game.com/' target="_blank">Try Concept Game</a></h4>
	</div>

	<div class="holder">
	<img class="holder_img" src="img/Flickr_imgs.png" alt="Flickr images"
	style=" width: 50%; /* Will shrink image to 30% of its original width */
			    height: auto; 
			    float: right;
			    padding-top: 30px;
  			    padding-left: 30px;">
	    <h3><b><a href='https://github.com/douwekiela/mmfeat' target="_blank"> MMFeat Flickr API</a></b></h3>
	    <p>
	    I developed a Python Flickr API for image search to <a href='http://www.cl.cam.ac.uk/~dk427/' target="_blank">Douwe Kiela</a>'s mmfeat<a href='https://github.com/douwekiela/mmfeat' target="_blank"> github project</a>.
	    It is a Python toolkit aiming to make it easier for researchers to use multi-modal features. Both image and sound (i.e., visual and auditory representations) are supported.
	    </p>
	</div>

</div> <!-- container -->

<hr />


<div class="container">
	<h2><a class="section" id="teaching">Teaching / Supervising</a></h2>
    <p>
    <strong>Lent 2017</strong> - I was demonstrating and ticking for <a href='https://www.cl.cam.ac.uk/teaching/1617/MLRD/' target="_blank">Machine Learning and Real-world Data</a>, A Part IA CST course at Computer Laboratory, University of Cambridge.
    </p>
    <p>
    <strong>2016/17</strong> - I co-supervised Christopher Davis, a Master student, on his thesis project about mutli-modal semantics. He got distinction and later a PhD position in our research group.
    </p>

</div> <!-- container -->

<hr />

<div class="container">
	<h2><a class="section" id="communal">Communal</a></h2>
	<p>
	<strong>2017/18</strong> -  I was a Mentoring Officer and Representative in the Cambridge Computer Laboratory's Graduate Forum of <a href="https://www.cst.cam.ac.uk/women" target="_blank">Women@CL</a>. Women@CL is an organisation to support women in computer science. It involves a mentoring system and provides representation of role models by organising talks and conferences.
	</p>
	<p>
	<strong>2017</strong> - I organised the <a href="http://talks.cam.ac.uk/show/index/6401" target="_blank">Cambridge NLIP Seminars</a>, which is a weekly seminar series in NLP and Machine Learning related topics.
	</p>

</div> <!-- container -->

<hr />

<div class="container">
	<h2><a class="section" id="awards">Awards</a></h2>
	<ul>
		<li>
			2023 - <a href="https://www.rsipvision.com/ComputerVisionNews-2023November/28/" target="_blank">Best of ICCV 2023 Magazine</a>, poster and extended abstract 
			Paris, France
		</li>
		
		<li>
			2014 - Award of the Challenge Handicap & Technologies 2014 - Reseau Telepresence
			system Nouvelles Technologies APF conference in Lille, France
		</li>
		<li>
			2014 - Best Student Video Award at AAAI Video Competition [<a href="http://aivideocompetition.org/telepresence-for-people-with-communication-impairment/" target="_blank">video</a>]
		</li>
		<li>
			2013 - National Conference of Student's Scholarly Circles, OTDK - Third Prize
		</li>
	</ul>

</div> <!-- container -->

<hr />

<div class="container">
	<div class="holder">
	<img class="holder_img" src="img/music.jpg" alt="guitar and mic"
	style=" width: 40%; /* Will shrink image to 30% of its original width */
			    height: auto; 
			    float: right;
			    padding-top: 30px;
  			    padding-left: 30px;
  			    padding-bottom: 10px;">
		<h2><a class="section" id="music_lit">Music</a></h2>
		<p>
		I have been singing since my childhood, starting as a member of the children choir of the Hungarian Opera House.
		</p>
		<p>
		Before I moved to the UK I was the lead singer (and occasional guitarist) of an alternative rock band with whom we made some <a href="https://www.youtube.com/user/BabyOnBoardzenekar" target="_blank">recordings</a>.
		</p>
		<p>
		You can listen to covers and my original songs on <a href="https://soundcloud.com/anita-ver" target="_blank">SoundCloud</a>
			or on my <a href="https://www.youtube.com/channel/UCZTluMPlZEEvPs7XezqHHjg" target="_blank">YouTube channel</a>.
		</p>
	</div>

	
	<div class="holder">
	<img class="holder_img" src="img/cassette of confinement.jpg" alt="poetry"
	style=" width: 40%; /* Will shrink image to 30% of its original width */
			    height: auto; 
			    float: right;
			    padding-top: 30px;
  			    padding-left: 30px;
  			    padding-bottom: 10px;">
		<h2><a class="section" id="music">Poetry</a></h2>
		<p>
		I recently attended <a href="https://www.coursera.org/learn/poetry-workshop" target="_blank"> this poetry writing workshop</a>, which I really enjoyed and learned a lot from.
		</p>
		<p>
		The poems I wrote for the workshop prompts are available in a mini anthology titled </br>
		"The Cassette of Confinement".  </br>
		It can be downloaded in <a href="The Cassette of Confinement.epub" target="_blank">ebup</a> and <a href="The Cassette of Confinement.pdf" target="_blank">PDF</a> formats.
		</p>
	</div>

</div> <!-- container -->

<div class="ribbon">
	<div class="attribution">
		<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">
			<img class="attribution-icon" src="img/cc_icon_white_x2.png" alt="Attribution 4.0 International (CC BY 4.0)">
		</a>
		<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank">
			<img class="attribution-icon" src="img/attribution_icon_white_x2.png" alt="Attribution 4.0 International (CC BY 4.0)">
		</a>
	</div>
</div>

</body>
</html>
